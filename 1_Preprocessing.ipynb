{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff738bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bd889",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "print('Dataset shape:', df.shape)\n",
    "print('\\nFirst few rows:')\n",
    "print(df.head())\n",
    "print('\\nData types:')\n",
    "print(df.dtypes)\n",
    "print('\\nMissing values:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b65dc6",
   "metadata": {},
   "source": [
    "## 2. Clean and Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ee7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TotalCharges to numeric (dataset may have non-numeric values)\n",
    "if 'TotalCharges' in df.columns:\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values created during conversion\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Drop customerID (identifier, not useful for modeling)\n",
    "if 'customerID' in df.columns:\n",
    "    df = df.drop(columns=['customerID'])\n",
    "\n",
    "print('After cleaning shape:', df.shape)\n",
    "print('Missing values after cleaning:')\n",
    "print(df.isnull().sum().sum(), 'total missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31757a86",
   "metadata": {},
   "source": [
    "## 3. Encode Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Churn: Yes -> 1, No -> 0\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "print('Target variable (Churn) distribution:')\n",
    "print(df['Churn'].value_counts())\n",
    "print('\\nChurn rate:', round(df['Churn'].mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9fc45",
   "metadata": {},
   "source": [
    "## 4. Identify Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target\n",
    "y = df['Churn']\n",
    "X = df.drop(columns=['Churn'])\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "print('Numeric columns:', len(numeric_cols))\n",
    "print(numeric_cols)\n",
    "print('\\nCategorical columns:', len(categorical_cols))\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fa576",
   "metadata": {},
   "source": [
    "## 5. Build Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines for each feature type\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "# Combine into single preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "print('Preprocessor created with:')\n",
    "print('  - Numeric transformer (StandardScaler) for:', numeric_cols)\n",
    "print('  - Categorical transformer (OneHotEncoder) for', len(categorical_cols), 'categorical features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8afe8e",
   "metadata": {},
   "source": [
    "## 6. Apply Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (stratify to keep churn proportions in both sets)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print('Train set size:', X_train.shape)\n",
    "print('Test set size:', X_test.shape)\n",
    "print('\\nTrain churn rate:', round(y_train.mean(), 4))\n",
    "print('Test churn rate:', round(y_test.mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4508c46",
   "metadata": {},
   "source": [
    "## 7. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data and metadata for next notebook\n",
    "import pickle\n",
    "\n",
    "# Save data splits\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'numeric_cols': numeric_cols,\n",
    "        'categorical_cols': categorical_cols\n",
    "    }, f)\n",
    "\n",
    "# Save preprocessor\n",
    "with open('preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "\n",
    "print('Saved: preprocessed_data.pkl')\n",
    "print('Saved: preprocessor.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
