{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba084e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import math\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                             roc_auc_score, mean_absolute_error, mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7626e26",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data from previous notebook\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "numeric_cols = data['numeric_cols']\n",
    "categorical_cols = data['categorical_cols']\n",
    "\n",
    "with open('preprocessor.pkl', 'rb') as f:\n",
    "    preprocessor = pickle.load(f)\n",
    "\n",
    "print('Loaded training set:', X_train.shape)\n",
    "print('Loaded test set:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed721b47",
   "metadata": {},
   "source": [
    "## 2. Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Logistic Regression pipeline\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "print('Training Logistic Regression...')\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620e4be",
   "metadata": {},
   "source": [
    "## 3. Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Random Forest pipeline\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print('Training Random Forest...')\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6445ead",
   "metadata": {},
   "source": [
    "## 4. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(pipeline, X_test, y_test, model_name='Model'):\n",
    "    \"\"\"Evaluate a model and print metrics.\"\"\"\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Get probabilities for ROC AUC\n",
    "    probs = None\n",
    "    try:\n",
    "        probs = pipeline.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    auc = None\n",
    "    if probs is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, probs)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {model_name} Evaluation\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"ROC AUC: {auc:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "    print(f\"\\nClassification Report:\\n{report}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'auc': auc,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'cm': cm,\n",
    "        'report': report,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14c1eb",
   "metadata": {},
   "source": [
    "## 5. Evaluate Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c73650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "lr_metrics = evaluate_model(lr_pipeline, X_test, y_test, 'Logistic Regression')\n",
    "rf_metrics = evaluate_model(rf_pipeline, X_test, y_test, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556fdcb",
   "metadata": {},
   "source": [
    "## 6. Model Comparison & Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf076db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  Model Comparison\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  Accuracy: {lr_metrics['accuracy']:.4f}\")\n",
    "print(f\"  ROC AUC:  {lr_metrics['auc']:.4f}\" if lr_metrics['auc'] else \"  ROC AUC: N/A\")\n",
    "\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"  Accuracy: {rf_metrics['accuracy']:.4f}\")\n",
    "print(f\"  ROC AUC:  {rf_metrics['auc']:.4f}\" if rf_metrics['auc'] else \"  ROC AUC: N/A\")\n",
    "\n",
    "# Determine best model\n",
    "if lr_metrics['accuracy'] >= rf_metrics['accuracy']:\n",
    "    best_model = lr_pipeline\n",
    "    best_name = 'Logistic Regression'\n",
    "    best_metrics = lr_metrics\n",
    "else:\n",
    "    best_model = rf_pipeline\n",
    "    best_name = 'Random Forest'\n",
    "    best_metrics = rf_metrics\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"âœ“ Recommended model: {best_name}\")\n",
    "print(f\"  Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47a367",
   "metadata": {},
   "source": [
    "## 7. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both pipelines\n",
    "joblib.dump(lr_pipeline, 'lr_model.joblib')\n",
    "joblib.dump(rf_pipeline, 'rf_model.joblib')\n",
    "joblib.dump(best_model, 'best_model.joblib')\n",
    "\n",
    "# Save metrics\n",
    "with open('model_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'lr_metrics': lr_metrics,\n",
    "        'rf_metrics': rf_metrics,\n",
    "        'best_model_name': best_name,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }, f)\n",
    "\n",
    "print('Saved: lr_model.joblib')\n",
    "print('Saved: rf_model.joblib')\n",
    "print('Saved: best_model.joblib')\n",
    "print('Saved: model_metrics.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
